# Logo-Detection---Attention-Overthrow
# Date: 12.12.2023
# Abstract
Within the world of image processing, there are numerous approaches commonly and widely used, spurred on by the advent of higher computational power, use of GPUs in Deep Learning, and, simply, innovation in CNNs over the years. To that end, we wanted to explore object detection. It’s an interesting aspect because while CNNs do regularly extract features from images turned into feature maps, object detection requires exacting precision where specific ‘features’ are concerned. We were intrigued by the idea of object detection in the context of the real world. In reality, most of the bleeding edge work in image processing is being done with utterly enormous datasets, which are often curated. Sure, they may have blurry or tough-to-decipher images within them, but not often the kinds of images people take. Many of us may think ourselves great photographers, but the photographs we often see taken by the average person, even with modern phones whose cameras nowadays rival professional grade titans, are typically quite poor. They are out of focus, poorly centered or framed, angled strangely, affected by phone movement, etc. However, up-to-date deep neural networks handle even such degenerate cases beautifully. In that respect, we found an interesting area to explore.

From this, we came up with the idea to use a dataset made up of the kinds of images real people would take. This makes the application more broad too, as at the end of the day while there are innumerable images taken by professional photographers using professional cameras, the vast, vast majority of existing image data in the world is those taken by effective amateurs, with meager quality. The disparity between the quantity of those high grade images and the lower quality ones taken by laymen, is only going to continue to grow as we move even more decades into the world where everyone has cameras in their pockets and an urge, coupled with strong incentive, to take pictures all the time. Such motivation was more than sufficient, and so we used such a dataset, containing images taken simply by hand, either by amateurs or in their style, which made up 6 different classes. This dataset was also rather unique in that it was quite small, containing only ~2300 images. This is territory which is not explored as much. The issue with training on small datasets tends to be that even SotA neural networks often overfit to them, and thus it does not seem the best idea for effective learning to use them. However for our purposes, it seemed a good enough niche to situate ourselves within.
 
We split that data between training and testing at a ~75/25 split, where we also broke down part of the training set further into a validation set. We wanted the model to learn effectively from the data, but do so in such a manner that it did not overfit. This was going to be the main challenge. However we have learned many techniques throughout the course to avoid this. Architecture-wise, there are many types of CNNs that seemed suitable for this. We started out using a ResNet architecture which we had carefully designed. We knew ResNets often performed well on smaller datasets too, which was an added bonus. However when we ran a few passes through the data with different epoch quantities and having tweaked the architecture to different degrees, we had limited success, we suspect due to the nature of the dataset’s size. So, we pivoted to a Vision Transformer architecture, as that was a novel area which neither of us had explored much. This too came with its own sets of problems. We had much trial and error, including very long training times gone to waste. But in the end, we were able to use this architecture to success, and we learned a ton about neural networks in the process. To boot, we gained experience with what appears to be the coming tsunami of the next eras of artificial intelligence. As the field grows to the mainstream, the transformer architecture seems to be the burgeoning head in several areas. Transformers have already broken the field of natural language processing (NLP) and even natural language understanding (NLU) wide open, and they are only going to improve (albeit maybe incrementally for a time) from here on out. 

Using a Vision Transformer (ViT) was mainly of particular interest due to how new they are. The transformer architecture itself was already quite new, having been introduced only in 2017. However, Vision Transformers are even newer, with their having been introduced, adapted from the base transformer toward Computer Vision tasks, in 2020, just 3 years prior to when we did this project. So while we had thought we devised a very interesting project in and of itself, we felt when using this new approach to the field of Image Processing, that we took it a step further, and made an even more interesting project. 

# Introduction: The Why

When considering how to approach our desired interest area, we had quite a few initial directions from which we thought about jumping off. We were toying at first with the idea of using a large logo dataset such as Logo2K+ or FlickrLogos-32 and then procuring another dataset which contained real world images and seeing whether the trained model could then identify whether given images contained any of the logos it had learned or not. This idea broke down when we got further in the planning phase, as we would have trouble finding two datasets which were compatible, and we felt much of the learning would go to waste if we just found a general dataset of real world images, as most of the logos would not appear. We did think about what the benefits could have been for this, but given we knew training would take such a long time with such an idea, it felt unwise to waste it. In that case, what we could have just done is taken those logo datasets and just done a basic model training/testing split. But we did not find this concept as interesting, it has been done countless times before and we have even done the same or similar things in this very class too. So we switched to a dataset containing real world images taken with someone’s phone, in public. The objects in question were logos for fast food brands. They ranged from large, to small, to being located wherever on the image. Sometimes there were none of the specific logos we were training for within an image, for these there was a separate class ‘Other’, the other 5 classes were ‘Starbucks’, ‘Subway’, ‘Burger King’, ‘KFC’, and ‘McDonalds’. The nature of these images and the size of these chains (and thus, the ubiquity of their logos) added a dimension of difficulty for the model to have to actually find the logos, whether they be behind someone’s head, on the side of a table, on a napkin, etc, and recognize what was common between the images. However, this was also something we expected a modern deep neural network to manage, with the right amount of data.

What made the dataset we chose even more interesting as a use case, was that it was small, much more than typically deemed worth training on by neural network practitioners. 

Such small datasets, as we mentioned before, often cause many models to overfit. As such, many practitioners will avoid using them altogether. When they do use them, as an exercise or for whatever niche purpose they have, they are often using the kinds of datasets we were perusing in our initial process - vector graphic images of logos, with quite large pixel counts, and high fidelity. We wanted to see what we could do with data that was the opposite of that in quality, while still retaining the property of being low sized.

The future is uncertain, of course, as it has been for all of human history. But we can be reasonably certain that AI is going to be a big part of it. Not every dataset we use for training going forward is going to be perfectly curated, clean, or look great. Often, they are going to be messy, blurry, poorly shot, and of inadequate size. What we are capable of doing with datasets of even these paltry quality, is going to be a bigger sign of how the world to come handles the AI revolution, than just how we can perform on perfect, massive, clean datasets of professionally shot photos or high-resolution graphics. As the men and women behind the AI themselves, we are going to need to do better, and fundamentally, that was what we wanted to try to do with this project.
# Implementation Overview: The How
As mentioned in the abstract, we were initially using a ResNet architecture. We had designed and fine-tuned it such that we were hoping for a good outcome. However, we found it was going to be an even bigger challenge than we’d initially thought. In all our initial testing, we ran into some issues, as evidenced by a few plots we can show:

![alt text](https://github.com/Abdumannon7/Logo-Detection---Attention-Overthrow/blob/a14dcdb8028b69a5b65a020056addd655f65fbdf/pictures/Train%20and%20Test%20Loss%20vs%20Epochs%20.png)

Here, we had ran our model for just 10 epochs to start with (even going forward from there we knew we had to keep our epochs number low, because with high enough epochs any model no matter how robust was going to definitely overfit due to just how small the dataset was), and while it was performing well at first, as you can see the error was dipping quite significantly in the early epochs, when we actually took derivatives and second derivatives of our output values and ran a few other metrics, we found that from this point onward the loss was actually appearing to be rebounding, and quite heavily at that which is not necessarily what we can see just by using the eye test. What is apparent from just the eye test, though, was that this model was indeed overfitting. While the eye test for these first several epochs appears to show the test loss going back down, when we ran a few more epochs afterward it became apparent that the losses for training and testing were not beginning to converge toward some low value, but were actually going back up and appearing to flatten and plateau afterward. This was discouraging enough that we cut off training and decided to tweak our architecture some more.
Some of the tweaks we tried gave us some errors, which ranged between causing what seemed to be a vanishing gradient (since we were not using a ReLu activation function which seems to be the standard way of avoiding vanishing gradients), standard compiler errors, data loading errors, and others. 

A few more such iterations where we ran into many different problems after tweaking architecture in many different ways and implementing quite a few different heuristics, changing logic, using different layer stacks, etc, and we decided that it might be time to try out a new architecture altogether. For object detection in image processing, there are many approaches we could have tried. We considered using YOLO, SSD, RetinaNet, Faster-R-CNN, among quite a few other approaches that appear to be high-performing, well-reputed, and sufficiently popular as to instill confidence in their robustness within us. However we kept landing on the fact that, from the research we did, ResNet was no worse than those when considering our computational resources and the size of the dataset we were using. So, we feared, no matter which of these other architectures we tried, even when we had thoroughly studied the way they worked and gotten enough practice with them that we felt comfortable using and modifying their implementations as we had done with the ResNet, we were going to end up running into the same issues.
That was when we realized we could only hope to have a realistic shot at improving, by either brute-forcing every one of these architectures (which could take too much time as we had already spent a month or so on the ResNet alone), we needed to branch out and try something radical and truly new. We came to two choices - the DRNA-Net (Discriminative Region Navigation and Augmentation Network), as proposed by researchers at Shandong Normal University in China, as they used it on their proprietary Logo2K+ dataset which they proposed within the same paper as the DRNA-Net - and the Vision Transformer. The DRNA-Net seemed sufficiently complex for us to use it and be able to gain something significant from modifying it. It is comprised of a few components, a feature extraction set of layers which has a Teacher Sub-network, whose job it is to extract features from logos and then pass them onto the next portion of the layer, the Navigator Sub-network, which converts the image into a feature map using the extracted features and then puts them through a ReLu activation function, some Max Pooling, and then stacks them a Feature Pyramid Network which hones in on the bases of the logos. Then they are passed onto the next component of the DRNA-Net, the Region-Oriented Data Augmentation Sub-network which is used for, as the name suggests, data augmentation, via another feature extractor through which they are passed into an augmentation map, that feeds into region dropping and region cropping portions. These decide which parts of features may be dropped or cropped in order for best detection, and from there they are passed onto the final component set of layers, the Scrutinizer Sub-network. This passes the data into yet another feature extractor, which fuses the features and regions, and uses vector operations to then feed them into the final layer which is fully connected and produces the prediction that the model has come up with. 
The paper provides a very strong foundation and framework to work off of and build our own adaptation or implementation of the concept. However, since this was already introduced specifically as an approach to logo recognition, it felt as though we were going to be stepping on some toes, or at the very least retreading ground which has been trod. So we decided on the novel approach of using Vision Transformers. This was a very new architecture, and we could not find examples of projects like ours being done with them before. As we talked about before, Transformers are a new architecture which were more known for how good they are with NLP and NLU tasks. ChatGPT has made them extraordinarily popular, and now there are countless similar AI chatbots. However, what’s not as well known is the capabilities it has to handle Computer Vision. The Vision Transformer, like the base Transformer, foregoes convolutions, pooling, recurrence, competitive layers, etc as we see in traditional neural network architectures. What Transformers have instead, are an encoder and a decoder, between which are self-attention layers. This is a very simplified overview of how they are designed, but these self-attention layers compute different types of error metrics, which are called “attention scores”. The encoder and decoder do contain feedforward neural networks within them, which may be an MLP or some other architecture, but the broader transformer is based mainly on the concept of having an encoder, a decoder, and self-attention layers. These attention scores combined with positional embedding, allow the Transformer to retain previous portions and contexts of input sequences such that they outperform even memory-augmented networks such as LSTM and GRU networks; which themselves were conceived due to the issue of traditional RNNs having problems with retaining information. The encoder takes an input sequence and breaks it into tokens (otherwise known as “subwords”), and the decoder produces an output sequence. They are trained to predict the next token given a certain token, and used to train over an unthinkably large dataset. Thus, we get products like ChatGPT, which can appear to ‘understand’ what we say to it. What it’s doing is using such extremely finely tuned statistical reasoning as to ‘predict’ the tokens of whatever inputs we give it, and ‘predict’ what output it can produce which would best answer the query it was given.  So all of this begs the question - how does a Transformer handle unstructured data such as images and videos, when the model was first introduced and used on a wide scale simply for NLP and NLU, and we do not know how it can even take in or interpret unstructured data? How does it deal with them, without using convolutional layers, which every other neural network structure we considered using for this project does? 
The answer, we found, was that it converts images into patches before passing them into the encoder!

From there, with the self-attention mechanism, they parse the features and use the decoder to reconstruct and predict from the patches. When it comes to NLP and NLU-oriented base Transformers, they can tend to be somewhat decoder-heavy, as the prediction and response are often the most important parts, and encoding is not quite as intensive. However when it comes to Vision Transformers, it is often the other way around, and they can be quite encoder-heavy, as the prediction portion is not as intensive, due to the added difficulty and dimensionality of intaking and interpreting the image data as compared to text. 
Now, when we first approached the model, it was not without its hiccups. We had a bunch of trial and error to get through our heads just how it works, until we felt we had a thorough enough understanding of the components that comprise it, to where we could use it. When we did start being able to run a model which was properly a Vision Transformer, we did have many such errors as we encountered before with the ResNet. However, we finally got one to run. Knowing how tough these can be to train, we started off with 100 epochs, however without even going to testing, we knew there was an issue: the validation error and accuracy was not what we wanted it to be, and our training accuracy and error indicated near perfect training. This means, thus, that it had clearly overfitted. Which was much to our dismay, given that it had taken so much time to train the model, however the results were quite definitive, as shown by the following plots we had the model generate:

![alt-text](https://github.com/Abdumannon7/Logo-Detection---Attention-Overthrow/blob/a14dcdb8028b69a5b65a020056addd655f65fbdf/pictures/trainign%20loss%20vs%20validation%20loss%20over%20epochs.png)

Showing how the Training loss went down to near 0, whereas the validation loss not only did not improve, it got so much worse that by the end of training their curves appear almost to be inverses of one another

![alt-text](https://github.com/Abdumannon7/Logo-Detection---Attention-Overthrow/blob/a14dcdb8028b69a5b65a020056addd655f65fbdf/pictures/accuracy%20vs%20epochs%20training%20and%20testing.png)

Showing how the training accuracy went up to where it was near perfect, whereas the validation accuracy had improved up to a certain point over time, but plateaued within the first 30-50 epochs or so and got to the point where it was near flat, almost unchanging, after a certain point, and quite a bit lower than where the training accuracy was. 
However, despite how poor this performance was, it was not as big of a discouraging factor as one may think. Because, we were just beginning to experiment with the model, and had just ran the first full training cycle which took us around as long as our work with the ResNet, and on top of this, we had somewhat more success on validation. Sure, the overfitting problem was not curbed, however validation accuracy around 80% was better than what we were getting before with the ResNet, which was nowhere near as good. 
This indicated to us that what we needed to do was augment our hyperparameters, use an annealing learning rate, learning rate scheduler, MultiStepLR procedure, or find some other way to maximize its efficacy. We also wanted to implement such processes as weight decay, and, importantly, an early stopping procedure, so we knew we never went too far with training. When it came to the previous cycle, it was evident that we could have stopped around epoch 50 and not lost much in terms of performance, so we could hope that cutting training off around there would work. 
So, with some elbow grease and brainstorming, we made some tweaks and went for another training cycle, replete with the changes we had considered. We knew this would likely take quite a bit of time to train yet again (the previous working iteration had taken over 12-16 hours to train despite the small dataset).
Here’s how we performed this time, after a few bug fixes we managed to squeeze out:

![alt-text](https://github.com/Abdumannon7/Logo-Detection---Attention-Overthrow/blob/a14dcdb8028b69a5b65a020056addd655f65fbdf/pictures/training%20loss%20vs%20validation%20loss.png)

One may see this chart and wonder why it had been cut off, that was because we had implemented an early stopping mechanism, which stopped the training around epoch 18. However, this did not seem appropriate to us, as from some mathematics we did and some other metrics, we could determine that it seemed the loss was going down, yet early stoppage kicked in anyway. This was upsetting, but we tweaked a bit more and went on. We made many changes to the structure and architecture we were using, we changed the learning rate modifier a few times, the best two were the Mult-StepLR, which was one of the final ones we tried, and the one we ended up with, which was the ReduceLROnPlateau optimizer which we found a good implementation for through the Keras documentation. We added in dropout and embedding dropout, which we had to tweak for a few iterations. We switched between a few loss functions such as the bounding box loss regression function, however we found that it was not working for us the way we wanted and after a few training cycles we swapped to cross entry loss. Which seemed to serve us better.
In terms of our work, to clean things up, we used 3 separate .py files, one for the hyperparameters which made a few of them easy to tweak whenever we wanted, one for training the model, it uses PyTorch’s built-in method to save the model once trained to a .pth file, and finally a file which contains the code to test the models. This was easier for us to use in training and served our purposes. 



# Results and Observations: The What
We want to show a bit of what the dataset looks like before we show what our results were on our most successful runs through training. 

These are, of course, utterly atrocious photographs. Which seem to have been taken at a shopping mall in Turkey. But that is precisely what makes the dataset a challenge for a neural network to learn on. It is a type of classification problem not often posed to AI with such meager datasets
In terms of our end results, we are not going to pretend as though the numbers we achieved were exactly what we had wanted. Where we reached was the best we had managed to do after many different iterations, training/testing cycles, and even a few entire architecture changes over the past few months. When viewing the data itself, it is not tough to see why a model can have trouble learning the patterns within it. Here was our accuracy plot for training:

![alt-text](https://github.com/Abdumannon7/Logo-Detection---Attention-Overthrow/blob/a14dcdb8028b69a5b65a020056addd655f65fbdf/pictures/training_and_validation_over_epochs.png)

We unfortunately were never able to get above around 88% accuracy for training and validation. Here, the training and validation ran pretty close after a certain point, we are not quite sure what caused the big dip in validation around epoch 7 or 8, but it was very early on. We assume it has to do with the learning rate and weight decay not setting in yet, and of course sufficient dropout had not occurred yet. Other than that, this plot does not indicate overfitting, as the others from prior iterations had. Beyond this, we generated quite a few more metrics for false positive/true positive rates, error, f1 score, recall, and whatnot. FP/TP are shown in our confusion matrix: 

![alt-text](https://github.com/Abdumannon7/Logo-Detection---Attention-Overthrow/blob/a14dcdb8028b69a5b65a020056addd655f65fbdf/pictures/Confussion_Matrix.png)

We had a nice ROC Curve to go with that:

![alt-text](https://github.com/Abdumannon7/Logo-Detection---Attention-Overthrow/blob/a14dcdb8028b69a5b65a020056addd655f65fbdf/pictures/ROC_curve.png)

Which was complemented well by our testing accuracy. We had good training and testing relative to each other, of course we would have preferred higher accuracy, however we believe we made the best attempt we could have with the given dataset being how small and tough for classification as it was. Here was our training and testing accuracy compared:
![alt-text](https://github.com/Abdumannon7/Logo-Detection---Attention-Overthrow/blob/a14dcdb8028b69a5b65a020056addd655f65fbdf/pictures/training%20and%20test%20accuracy%20vs%20Epochs.png)

While in a few respects some may say that this was a failure, we do not believe it to be so. We achieved quite decent accuracy and remarkably tight testing and training curves despite the nature of the dataset we used. 
In conclusion, we believe that with this project, we managed to show the power of the transformer model, as even inexperienced practitioners who had to teach ourselves how it worked and were using a new novel version of the architecture which was previously only used for NLP and NLU before the Vision Transformer was introduced just a few years ago. We were dealing with a very difficult dataset, a type which was not much explored prior, and an architecture with which we were not familiar, yet we achieved decent results anyway. We hope to explore Transformers and other such novel architectures in the future, as the biggest driving force of AI is innovation - the now is constantly changing, and it is only by matching that ceaseless advancement with equal daring and curiosity that we can keep up with the wave of the future.




